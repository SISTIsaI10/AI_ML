{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Linkedin: [https://www.linkedin.com/in/adrian-sanchez-hernandez/](https://www.linkedin.com/in/adrian-sanchez-hernandez/)\n\n# Employee Burnout Prediction\n\n\nEmployee burnout is a state of physical, emotional and mental exhaustion caused by excessive and prolonged stress. It can have serious consequences on an individual's well-being and can lead to decreased productivity and job performance. In today's fast-paced and constantly connected world, it is increasingly important to recognize and address the signs of burnout in order to maintain the health and well-being of employees.\n\nAs part of a final project for the course Supervised Machine Learning: Regression, from IBM Machine Learning profesional certificate, we will be exploring the use of regression techniques to predict employee burnout. By analyzing a dataset containing various factors that may contribute to burnout such as workload, mental fatigue job and work-life balance, we can develop a model to identify individuals who may be at risk of burnout. By proactively addressing these risk factors, organizations can help prevent burnout and promote the well-being of their employees.\n\n## Dataset: Are Your Employees Burning Out?\nThis [dataset](https://www.kaggle.com/datasets/blurredmachine/are-your-employees-burning-out?select=train.csv) consists of 9 columns as follows:\n\n* **Employee ID**: The unique ID allocated for each employee **(example: fffe390032003000)**\n* **Date of Joining**: The date-time when the employee has joined the organization **(example: 2008-12-30)**\n* **Gender**: The gender of the employee **(Male/Female)**\n* **Company Type**: The type of company where the employee is working **(Service/Product)**\n* **WFH Setup Available**: Is the work from home facility available for the employee **(Yes/No)**\n* **Designation**: The designation of the employee of work in the organization.\nIn the **range of [0.0, 5.0]** bigger is higher designation.\n* **Resource Allocation**: The amount of resource allocated to the employee to work, ie. number of working hours.\nIn the **range of [1.0, 10.0]** (higher means more resource)\n* **Mental Fatigue Score**: The level of fatigue mentally the employee is facing.\nIn the **range of [0.0, 10.0]** where 0.0 means no fatigue and 10.0 means completely fatigue.\n* **Burn Rate**: The value we need to predict for each employee telling the rate of Bur out while working.\nIn the **range of [0.0, 1.0]** where the higher the value is more is the burn out.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Exploratory data analysis (EDA) is an important step in the machine learning process, as it allows us to gain insights into the characteristics and patterns of our data.\n\nFor this, we need first to load the data:","metadata":{}},{"cell_type":"code","source":"import warnings\n\ndef _warn(*argv, **kwargs):\n    pass\n\nwarnings.warn = _warn\n\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\ndf = pd.read_csv(\"/kaggle/input/are-your-employees-burning-out/train.csv\")\n\n# Remove spaces in columns names for accessing by properties\ndf.columns = [c.replace(' ', '_') for c in df.columns]\n\n# Remove Employee ID on dataframe showed\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:31.907255Z","iopub.execute_input":"2023-01-03T17:21:31.907598Z","iopub.status.idle":"2023-01-03T17:21:31.9723Z","shell.execute_reply.started":"2023-01-03T17:21:31.907572Z","shell.execute_reply":"2023-01-03T17:21:31.971194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the table shows, this dataframe contains 22750 columns where each one represents an observation of our true data. Also, as we can see in the column \"Mental Fatigue Score\" there are also NaN values. Let's count them.","metadata":{}},{"cell_type":"markdown","source":"### NaN values","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:31.974651Z","iopub.execute_input":"2023-01-03T17:21:31.975104Z","iopub.status.idle":"2023-01-03T17:21:31.988959Z","shell.execute_reply.started":"2023-01-03T17:21:31.975041Z","shell.execute_reply":"2023-01-03T17:21:31.987824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are NaN values on our target (\"Burn Rate\") and also in the last two columns. As we are going to perform supervised linear regression, our target variable is needed to do so. Therefore, this 1124 rows with NaN values must be dropped off of our dataframe.\n\nFurthermore, the other columns with NaN values, prior to any analysis, seem to have an important relation with the target. This can be appreciated computing the pearson correlation.","metadata":{}},{"cell_type":"code","source":"df.corr().Burn_Rate[:-1]","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:31.990037Z","iopub.execute_input":"2023-01-03T17:21:31.990311Z","iopub.status.idle":"2023-01-03T17:21:31.999233Z","shell.execute_reply.started":"2023-01-03T17:21:31.990286Z","shell.execute_reply":"2023-01-03T17:21:31.998351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we said, this variables are strongly correlated with our target and, therefore, important to estimate it. This correlation can also be seen plotting the relationship within the variables.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.pairplot(df)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:32.001705Z","iopub.execute_input":"2023-01-03T17:21:32.001968Z","iopub.status.idle":"2023-01-03T17:21:36.275831Z","shell.execute_reply.started":"2023-01-03T17:21:32.001943Z","shell.execute_reply":"2023-01-03T17:21:36.274778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For all this, in this case, we will drop off all observations with NaN values of our dataframe.","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.277273Z","iopub.execute_input":"2023-01-03T17:21:36.277834Z","iopub.status.idle":"2023-01-03T17:21:36.295056Z","shell.execute_reply.started":"2023-01-03T17:21:36.277797Z","shell.execute_reply":"2023-01-03T17:21:36.294233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have 18590 entries instead of 22750. The next thing we are going to do is to analyze what type of data is each variable.","metadata":{}},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.296243Z","iopub.execute_input":"2023-01-03T17:21:36.296758Z","iopub.status.idle":"2023-01-03T17:21:36.303405Z","shell.execute_reply.started":"2023-01-03T17:21:36.296732Z","shell.execute_reply":"2023-01-03T17:21:36.302541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the first 5 columns have non-numeric values that have to be treated. Before this, let's see what are the values that each variable contains.","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\ndf_obj = df.select_dtypes(object)\n\n# prints a dictionary of max 10 unique values for each non-numeric column\npprint({ c : df_obj[c].unique()[:10] for c in df_obj.columns})","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.304591Z","iopub.execute_input":"2023-01-03T17:21:36.305011Z","iopub.status.idle":"2023-01-03T17:21:36.327456Z","shell.execute_reply.started":"2023-01-03T17:21:36.304984Z","shell.execute_reply":"2023-01-03T17:21:36.326482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, we have categorical values for columns 'Company_Type', 'Gender', 'WFH_Setup_Available', specifically binary classes that we'll treat later. Also we have the IDs of the employees, which doesn't provide any useful information and, therefore, they must be dropped. ","metadata":{}},{"cell_type":"code","source":"df = df.drop(\"Employee_ID\", axis=1)\ndf.head().T","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.328947Z","iopub.execute_input":"2023-01-03T17:21:36.329543Z","iopub.status.idle":"2023-01-03T17:21:36.343804Z","shell.execute_reply.started":"2023-01-03T17:21:36.329515Z","shell.execute_reply":"2023-01-03T17:21:36.343127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we have 'Date_of_Joining' which contains dates on year 2008.\n\nPrior to any analysis, the date of joining doesn't seem that could have a direct relationship with our target. So, before discarding it, let's first transform this variable into another numeric one with which to analyze if it has any correlation with our target variable.\n\nFor this, let's see first its data distribution.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nprint(f\"Min date {df.Date_of_Joining.min()}\")\nprint(f\"Max date {df.Date_of_Joining.max()}\")\ndf_month = df.copy()\n\ndf_month[\"Date_of_Joining\"] = df_month.Date_of_Joining.astype(\"datetime64\")\ndf_month.Date_of_Joining.groupby(\n        df_month.Date_of_Joining.dt.month\n    ).count().plot(kind=\"bar\", xlabel='Month', ylabel = \"Hired employees\")\n","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.34484Z","iopub.execute_input":"2023-01-03T17:21:36.345464Z","iopub.status.idle":"2023-01-03T17:21:36.594065Z","shell.execute_reply.started":"2023-01-03T17:21:36.345434Z","shell.execute_reply":"2023-01-03T17:21:36.593136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the date of joining is uniform distributed with values between 2008-01-01 and 2008-12-31. So in order to create a new feature which represents the labor seniority, we could create a variable with de days worked. But, since we don't have the information of when this observations are made, let's first parse the date to the day of the year when the employee was hired and see if there is any correlation with our target data.\n\nFor this, we will count the days from 2008-01-01 until the date of joining:","metadata":{}},{"cell_type":"code","source":"dt_2008 = pd.to_datetime([\"2008-01-01\"]*len(df))\ndf[\"Days\"] = df.Date_of_Joining.astype(\"datetime64\").sub(dt_2008).dt.days\ndf.Days","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.598102Z","iopub.execute_input":"2023-01-03T17:21:36.598417Z","iopub.status.idle":"2023-01-03T17:21:36.619191Z","shell.execute_reply.started":"2023-01-03T17:21:36.598389Z","shell.execute_reply":"2023-01-03T17:21:36.618341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, as we did before, let's perform the pearson correlation again.","metadata":{}},{"cell_type":"code","source":"df.corr().Burn_Rate[df.select_dtypes(exclude=object).columns != \"Burn_Rate\"]","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.620438Z","iopub.execute_input":"2023-01-03T17:21:36.620875Z","iopub.status.idle":"2023-01-03T17:21:36.632036Z","shell.execute_reply.started":"2023-01-03T17:21:36.620839Z","shell.execute_reply":"2023-01-03T17:21:36.631129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the day of the year when the employee was hired doesn't have any correlation with our target. So in this case, rather than creating features from the original date of joining variable, we will prescind of it.","metadata":{}},{"cell_type":"code","source":"df = df.drop([\"Date_of_Joining\", \"Days\"], axis=1)\ndf.head().T","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.633226Z","iopub.execute_input":"2023-01-03T17:21:36.634062Z","iopub.status.idle":"2023-01-03T17:21:36.647673Z","shell.execute_reply.started":"2023-01-03T17:21:36.634026Z","shell.execute_reply":"2023-01-03T17:21:36.646696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With respect to the categorical variables that we haven't treated yet, it's interesting to see first how its different classes are distributed and how they affect the other variables.","metadata":{}},{"cell_type":"code","source":"cat_columns = df.select_dtypes(object).columns\nfig, ax = plt.subplots(nrows=1, ncols=len(cat_columns), sharey=True, figsize=(10,5))\nfor i, c in enumerate(cat_columns):\n    sns.countplot(df[c], ax=ax[i])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.648827Z","iopub.execute_input":"2023-01-03T17:21:36.649195Z","iopub.status.idle":"2023-01-03T17:21:36.974996Z","shell.execute_reply.started":"2023-01-03T17:21:36.649161Z","shell.execute_reply":"2023-01-03T17:21:36.97412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of observations of each category on each variable is equally distributed, except to the Company_Type where the number of service jobs its almost twice that of product ones.","metadata":{}},{"cell_type":"code","source":"for c in df.select_dtypes(object).columns:\n    sns.pairplot(df, hue=c)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:21:36.976337Z","iopub.execute_input":"2023-01-03T17:21:36.976632Z","iopub.status.idle":"2023-01-03T17:22:22.940775Z","shell.execute_reply.started":"2023-01-03T17:21:36.976604Z","shell.execute_reply":"2023-01-03T17:22:22.940075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, if working from home is available, the \"Burn_Rate\" and \"Mental_Fatigue\" tend to have lower values, but also in \"Resource_Allocation\". Same is slightly perceived when the gender of the employee is female.\n\nOther things to considerate are that the distributions of the features and target are normally distributed and not skewed, which save them, specifically the target, of further treatment for fixing this. We can check this normality with the statistical test of normality based on D’Agostino and Pearson’s test:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import normaltest\n\nfor c in df.select_dtypes(exclude=object):\n    print(f\"{c}: (p-value = {normaltest(df[c])[1]})\")","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:22.941887Z","iopub.execute_input":"2023-01-03T17:22:22.942454Z","iopub.status.idle":"2023-01-03T17:22:22.955992Z","shell.execute_reply.started":"2023-01-03T17:22:22.942421Z","shell.execute_reply":"2023-01-03T17:22:22.955066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can say that the variables are normally distributed since all p-values<.05.","metadata":{}},{"cell_type":"markdown","source":"### One-Hot Encoding\nNext, what we are going to do is to parse this classes to categorical values using the sparse representation with unit distance between classes known as one-hot encoding. Since all our categorical features are binary, and we don't want to represent multilinearity in our data, this classes can be represented with one dimension for each feature.","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=[\"Company_Type\", \"WFH_Setup_Available\", \"Gender\"], drop_first=True)\ndf.head().T","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:22.957124Z","iopub.execute_input":"2023-01-03T17:22:22.957388Z","iopub.status.idle":"2023-01-03T17:22:22.98053Z","shell.execute_reply.started":"2023-01-03T17:22:22.957363Z","shell.execute_reply":"2023-01-03T17:22:22.97981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, our dataframe doesn't have non-numeric values and all previous them are represented with just a numeric column.\nBefore continuing to use data pipelines and models to fit our data, we need first to split our dataframe in our input features X and target value y.","metadata":{}},{"cell_type":"markdown","source":"### X and y data","metadata":{}},{"cell_type":"code","source":"y_col = 'Burn_Rate'\ndf.columns\nX = df[[c for c in df.columns if c != y_col]]\ny = df[y_col]\n\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:22.981667Z","iopub.execute_input":"2023-01-03T17:22:22.982072Z","iopub.status.idle":"2023-01-03T17:22:22.989019Z","shell.execute_reply.started":"2023-01-03T17:22:22.982045Z","shell.execute_reply":"2023-01-03T17:22:22.988013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline models and Machine Learning","metadata":{}},{"cell_type":"markdown","source":"In this Section we will create data pipelines with models to find which one perform better on our data.\nFor this, we are going to first explain the process and components that we will use.","metadata":{}},{"cell_type":"markdown","source":"### Inference\nThe way that we are going to measure the performance of our models is using the [R² metric](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n\nAlso, since we want also to fine-tune the hyperparameter our models to perform better on unseen data, we'll be using Cross Validation for this and also for selecting the best model.\n\nAbout the models that we are going to use are:\n* Linear Regression\n* Lasso Regression\n* Ridge Regression\n\nThe first one simply fits the parameters of the weighted sum of the input data that the linear function performs. The difference between this one and the second ones are that Lasso and Ridge introduce a regularization penalty for having big values in the fitted parameters (to avoid the overfitting phenomenon), which is linear respect to the parameters in the case of Lasso regression and quadratic for Ridge.\n\nNotice that all these models are linear, since all of them performs a linear combination of the input features and weights. To make this linear models capable of approximating non-linear distributions, we will use a technique called polynomial features to represent the non-linear relations between the variables through the input data. Although we saw in the plots that our variables have a very linear relationship with our target, there might be some non-linear as with the \"Mental_Fatigue_Score\" feature which seems to have an small quadratic component. \n\nLast, since the regularization penalty of the parameters associated with each input feature depends on its magnitude, this inputs should be on a similar scale in order to our models perform well, especially if we use polynomials features. For this, we will use the standard scaler (although exist other ones), which consists of subtracting the mean and dividing by the variance to have zero mean and unit variance.\n\nSummarizing, the pipeline will consist of the following steps:\n1. Create polynomial features\n2. Use standard scaling\n3. Fit one of the linear models\n4. Measure the performance of the model on unseen data.\n\nSo, for adjusting the hyperparameters to perform better on unseen data and for doing model selection, we will use the defined pipeline combined with grid search and cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import make_scorer\nfrom typing import Dict\n\ndef create_pipeline(model_name: str, model_params: Dict):\n    \"\"\"Creates a pipeline with its parameters using the\n    model class name and a dictionary of parameters.\n\n    Args:\n        model_name (str): Sklearn linear model class name\n        model_params (Dict): Parameters of the model\n\n    Returns:\n        Tuple: Pipeline, parameters\n    \"\"\"\n    model = getattr(linear_model, model_name)()\n    estimator = Pipeline([\n            (\"polynomial_features\", PolynomialFeatures()),\n            (\"scaler\", StandardScaler()),\n            (model_name, model)])\n\n    params = {\n        'polynomial_features__degree': [1, 2],\n    }\n    params.update(\n        { model_name + \"__\" + k : v for k,v in model_params.items()}\n    )\n    \n    return estimator, params","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:22.990332Z","iopub.execute_input":"2023-01-03T17:22:22.990898Z","iopub.status.idle":"2023-01-03T17:22:22.998888Z","shell.execute_reply.started":"2023-01-03T17:22:22.990862Z","shell.execute_reply":"2023-01-03T17:22:22.997751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = {}\n\nfor name, params in {\n    \"LinearRegression\": {},\n    \"Lasso\": { \"alpha\" : np.geomspace(0.0001, 0.001, 50)},\n    \"Ridge\": { \"alpha\" : np.geomspace(10, 15, 50)},\n}.items():\n    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n    estimator, params = create_pipeline(name, params)\n    \n    grid = GridSearchCV(estimator, params, cv=kf,\n                        scoring = make_scorer(r2_score), n_jobs=4)\n    result = grid.fit(X, y)\n    \n    print(name)\n    print(\"=\"*len(name))\n    print(\"Params:\")\n    pprint(result.best_params_)\n    print(f\"R2 score: {result.best_score_}\")\n    print()\n    \n    results[name] = result","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:23.000083Z","iopub.execute_input":"2023-01-03T17:22:23.000422Z","iopub.status.idle":"2023-01-03T17:22:47.911757Z","shell.execute_reply.started":"2023-01-03T17:22:23.000397Z","shell.execute_reply":"2023-01-03T17:22:47.910116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the regularized models perform slightly better than the linear regression and the results within the Lasso model and Ridge are quite similar.\n\nNow, we have several model pipelines fitted and fine-tuned for predicting the burning rate of an unknown employee. We can also fit our best model with all our data hoping to perform better. For doing so, we will use the Lasso model.","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\nestimator = deepcopy(results[\"Lasso\"].best_estimator_)\n\nestimator.fit(X, y)\nprint(r2_score(y, estimator.predict(X)))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:47.914556Z","iopub.execute_input":"2023-01-03T17:22:47.915611Z","iopub.status.idle":"2023-01-03T17:22:48.190155Z","shell.execute_reply.started":"2023-01-03T17:22:47.915549Z","shell.execute_reply":"2023-01-03T17:22:48.188694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we have the model that the company needs.","metadata":{}},{"cell_type":"markdown","source":"### Interpretation","metadata":{}},{"cell_type":"markdown","source":"In order to see which input features are most important for predicting the Burn Rate, we have to analyze the coefficients of the models. First, we are going to describe the main statistics associated with them.","metadata":{}},{"cell_type":"code","source":"df_coefs = pd.DataFrame()\nfor model_name, result in results.items():\n    df_coefs[model_name+\"_Coefficients\"] = result.best_estimator_[-1].coef_\n\ndf_coefs_stats = df_coefs.describe()\ndf_coefs_stats.rename({'50%': 'median'}, inplace=True)\ndf_coefs_stats","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:22:48.196778Z","iopub.execute_input":"2023-01-03T17:22:48.200347Z","iopub.status.idle":"2023-01-03T17:22:48.286316Z","shell.execute_reply.started":"2023-01-03T17:22:48.200283Z","shell.execute_reply":"2023-01-03T17:22:48.284881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing we can note it's that the coefficients of the linear model are higher than the regularized models ones. This is, in fact, produced by the regularization penalty mentioned earlier what makes the second ones have smaller values.\n\nSince this values are quite high, small changes in the inputs (which have non-linear relations) can produce big changes in the output, not generalizing well on unseen data due to its variance. This is exactly what it's been trying to avoid with this regularized models.\n\nAnother thing to note is that most frequent coefficient (median) in the Lasso model is 0. This is because this model tends to cancel not necessary features, performing automatically feature selection which is useful for our actual purpose. So let's count the number of zero-valued coefficients:","metadata":{}},{"cell_type":"code","source":"estimator = results[\"Lasso\"]\ncoefs = estimator.best_estimator_[-1].coef_\npf_names = estimator.best_estimator_[0].get_feature_names_out(X.columns)\npf_names = [col.replace(\" \", \" * \") for col in pf_names]\n\nprint(\"Total Coefficients: \", len(coefs))\nprint(\"Number of zero-value coefficients: \", (coefs==0).sum())","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:27:00.1954Z","iopub.execute_input":"2023-01-03T17:27:00.195805Z","iopub.status.idle":"2023-01-03T17:27:00.204113Z","shell.execute_reply.started":"2023-01-03T17:27:00.195771Z","shell.execute_reply":"2023-01-03T17:27:00.202961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, 11 polynomial features of the 28 doesn't have any predictive power for our problem. Now let's sort the coefficients to see which them are the most correlated with our target variable.","metadata":{}},{"cell_type":"code","source":"df_coefs = pd.DataFrame(coefs).T\ndf_coefs.columns = pf_names\ndf_coefs.T.sort_values(0, ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T17:27:03.603631Z","iopub.execute_input":"2023-01-03T17:27:03.604014Z","iopub.status.idle":"2023-01-03T17:27:03.617141Z","shell.execute_reply.started":"2023-01-03T17:27:03.603969Z","shell.execute_reply":"2023-01-03T17:27:03.616019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To conclude, the most important variables that influences the burnout syndrome of an employee are mainly the mental fatigue and the amount of resource allocated (i.e. hours of work). Also, the main thing that seems to have a positive impact on reducing this effect is to have the opportunity of working from home.\n\n### Next steps\nAs the number of most important features is reduced, as we saw with the zero-valued coefficients of the lasso model, we could perform PCA over the polynomial features, eliminating half of them and preserving likely almost 99% of the variance. This will make our predictions faster although it doesn't seem to be a problem in this case.\n\nOther things to prove is trying other linear models and more complex ones, although the relationship within the input features and target is very linear. For this, we could try:\n\n* Multivariate Adaptive Regression Splines\n* Deep Neural Network\n* KNN Regressor\n* Decision Tree Regressor\n* Random Forest Regressor\n* Gradient Boosting Regressor\n\n\n# Conclusion\n\nIn this project, which was part of the final project for the course Supervised Machine Learning: Regression from the IBM Machine Learning Professional Certificate, we created a python notebook to predict employee burnout using burn rate regression and EDA. The features we used included mental fatigue score, work from home status, and workload. To prepare the data for analysis, we treated null values and encoded categorical variables. We also studied correlations and applied techniques such as grid search cross validation and polynomial features to the data. We then used the R2 metric to evaluate the performance of our model. Overall, the goal of this project was to build a model that could accurately predict burnout risk and potentially be used by organizations to prevent burnout and promote the well-being of their employees.\n\nI hope you found useful this notebook, both the code and the explanations.\n\nBest regards,\n\n[Adrian](https://www.linkedin.com/in/adrian-sanchez-hernandez/)\n","metadata":{}}]}